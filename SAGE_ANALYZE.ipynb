{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Notebook\n",
    "\n",
    "This notebook is used to analyze datasets using trained models as well as other imported segmentations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[1. Environment Initialization and Setup](#1-environment-initialization-and-setup)  \n",
    "- [1.1 Configurations](#11-configurations)  \n",
    "\n",
    "[2. Loading Datasets](#2-loading-datasets)  \n",
    "\n",
    "[3. Inference and Visualization](#3-inference-and-visualization)  \n",
    "- [3.1 Load Models](#31-load-models)  \n",
    "- [3.2 Save Mask Overlays](#32-save-mask-overlays)  \n",
    "  - [3.2a Dataset Masks](#32a-dataset-masks)  \n",
    "  - [3.2b Model Prediction Masks](#32b-model-prediction-masks)  \n",
    "- [3.3 IoU Heatmaps](#33-iou-heatmaps)  \n",
    "\n",
    "[4. Evaluation](#4-evaluation)  \n",
    "- [4.1 Comparing IoU Distributions of multiple models](#41-comparing-iou-distributions-of-multiple-models)  \n",
    "- [4.2 Postprocessing/Results](#42-postprocessingresults)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Initialization and Setup\n",
    "\n",
    "This cell prepares the runtime environment and project structure for evaluating our models:\n",
    "\n",
    "- **GPU Selection and Logging**\n",
    "    - `os.environ[\"CUDA_VISIBLE_DEVICES]`: choose which GPU to use for this notebook (here, GPU 3). This allows us to run this notebook while other notebooks (such as `SAGE_train.ipynb`) run on other GPUs.\n",
    "    - `os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]`: suppress TensorFlow info/warning messages\n",
    "    - `warnings.filterwarnings(\"ignore\", message=\".Model.state_updates.*\")`: suppresses warnings related to state updates for cleaner output\n",
    "\n",
    "It also sets various directories that need to be accessed throughout the notebook, as well as handles necessary package imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Set which GPU to use dynamically (e.g., \"4\" for GPU 4)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# Suppress tensorflow WARNING and INFO logs, shows only ERRORs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" \n",
    "\n",
    "# Supress model state warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Model.state_updates.*\")\n",
    "\n",
    "import tensorflow as tf\n",
    "# Uncomment for debugging/TF build\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "# print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "# print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "#==============================================================\n",
    "# CORE Libraries\n",
    "#==============================================================\n",
    "#TODO: cleanup unused imports that are now in other .py files, or make a single __init__ file\n",
    "import random, math, re, time, csv, tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "#Fractal Dimension analyzer\n",
    "import StereoFractAnalyzer as SF\n",
    "\n",
    "\n",
    "#============================================================\n",
    "# Project Directory Setup\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../SAGE\")\n",
    "print(\"ROOT DIR:\", ROOT_DIR)\n",
    "\n",
    "# Subdirectories\n",
    "\n",
    "PRETRAIN_DIR = os.path.join(ROOT_DIR,\"pretrained_models\") #Pretrained models folder, where models already trained are stored\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\") # Directory to save logs and trained models\n",
    "Results_DIR = os.path.join(ROOT_DIR, \"Results\") #directory to save morphology results, performance metrics, and visualizations\n",
    "#TODO: remove for final version\n",
    "DATA_DIR = os.path.abspath(os.path.join(ROOT_DIR, \"../../Data/logs\")) # Using this to save models into for storage on disk \n",
    "\n",
    "\n",
    "print(\"Pretrained models dir:\", PRETRAIN_DIR)\n",
    "print(\"MODEL_DIR: \", MODEL_DIR)\n",
    "print(\"Results DIR:\", Results_DIR)\n",
    "print(\"DATA DIRECTORY:\", DATA_DIR) #TODO: remove for final push\n",
    "\n",
    "# ================================================\n",
    "# Mask RCNN Setup\n",
    "# ================================================\n",
    "\n",
    "\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils, visualize\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn.utils import print_verbose\n",
    "\n",
    "mrcnn_dir = os.path.dirname(modellib.__file__)\n",
    "model_file_path = os.path.join(mrcnn_dir,'model.py')\n",
    "print(\"mrcnn directory:\",mrcnn_dir)\n",
    "print(\"Path to model.py:\", model_file_path)\n",
    "\n",
    "#Ensure COCO weights are available\n",
    "COCO_MODEL_PATH = os.path.join(PRETRAIN_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "#TODO: use similar method for downloading pretrained models/images as COCO?\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Configurations\n",
    "\n",
    "Here, we create a subclass of the base Config class for our SAGE model. We can specify how many GPUs to train on (1 in this case, due to hardware limitations), images per gpu, as well as how many classes (including background class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"SAGE\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1 #+ 1# background + particle + cluster\n",
    "\n",
    "  # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 1024\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further\n",
    "    # up scaling. For example, if set to 2 then images are scaled up to double\n",
    "    # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.\n",
    "    # However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "    # Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4\n",
    "    # Changing this requires other changes in the code. See the WIKI for more\n",
    "    # details: https://github.com/matterport/Mask_RCNN/wiki\n",
    "    IMAGE_CHANNEL_COUNT = 3 #images are grayscale(8bit) so may need to change to 1\n",
    "    \n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9]) #may need to change to one value for grayscale\n",
    "\n",
    "    # Default \n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH= 76# 76 for 200\n",
    "\n",
    "    #non-maximum suppression threshold for detection\n",
    "    DETECTION_MIN_CONFIDENCE = 0.6\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 25\n",
    "    \n",
    "    #EARLY STOPPING\n",
    "    EARLY_STOPPING_MONITOR = 'val_loss'\n",
    "    EARLY_STOPPING_PATIENCE = 20 #number of epochs with no improvement required to stop\n",
    "    ES_RESTORE_BEST_WEIGHTS =True\n",
    "    ES_MODE= \"min\"\n",
    "    ES_VERBOSE = 0\n",
    "    \n",
    "config = SAGEConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Datasets\n",
    "\n",
    "Here we will load the TEM images that we want to analyze using a model. \n",
    "\n",
    "To compare model performance with other segmentation methods, we can also load any dataset that contains the images and masks created with that method (in the same format as the images and masks used for training)\n",
    "\n",
    "Multiple datasets can be loaded into the `datasets` dictionary for easy access or comparison.\n",
    "\n",
    "As in the training notebook, they are loaded using the `SAGE_Dataset` class\n",
    "\n",
    "Define datasets to load in a list of tuples, containing:\n",
    "1. The dataset name (i.e., `PROCI_Test`)\n",
    "2. A boolean `use_results` indicating whether or not a results directory should be created.\n",
    "\n",
    "In this example, we load two datasets:\n",
    "* `PROCI_Test`: Real TEM images of soot that have been manually segmented by various analysts, used to determine the performance of our models and other segmentation methods.\n",
    "* `PROCI_EDMWS`: The same TEM images as in `PROCI_Test`, but with masks created using an EDM-WS Method from [Sipkens' atems MATLAB tool](https://github.com/tsipkens/atems). We will treat this dataset as \"predictions\" to check against the `PROCI_Test` \"ground truths\".\n",
    "\n",
    "As we are using these datasets for analysis purposes, set `use_results=True` to create corresponding results directories. \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#List of tuples: (dataset_name, use_results)\n",
    "\n",
    "\n",
    "datasets_to_load= [\n",
    "    ('PROCI_Test', True),\n",
    "    ('PROCI_EDMWS', True),\n",
    "   \n",
    "]\n",
    "datasets = {}\n",
    "\n",
    "\n",
    "for idx, (name, use_results) in enumerate(datasets_to_load, start=1):\n",
    "    clear_output(wait=True)\n",
    "    pbar_datasets = tqdm(total=len(datasets_to_load), desc=\"Loading datasets\", dynamic_ncols=True, position=0, leave=True, initial=idx-1,bar_format=\"{l_bar}{bar} {n_fmt}/{total_fmt}\")\n",
    "\n",
    "    datasets[name] = utils.load_and_register_dataset(name,ROOT_DIR, Results_DIR, create_dirs=use_results)\n",
    "   \n",
    "    pbar_datasets.update(1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as in the training notebook, we will verify that the masks are correctly assigned for each dataset using `visualize.display_top_masks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "#image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n--- Dataset: {name} ---\")\n",
    "    if len(dataset.image_ids) ==0: \n",
    "        print(\"No images loaded\")\n",
    "        continue\n",
    "    image_id = dataset.image_ids[0]\n",
    "    print(f\"Image ID:{image_id}\")\n",
    "    \n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    \n",
    "    print(f\"Mask shape for Image ID {image_id}: {mask.shape}\")\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference and Visualization\n",
    "\n",
    "## 3.1 Load Models\n",
    "\n",
    "After loading our datasets, we will load in our trained models. For ease of use, multiple models can be loaded by adding their path to the `model_paths` list. In this example, we are using our previously trained SAGE and COCO models, located in the `pretrained_models` directory.\n",
    "\n",
    "Model Information:\n",
    "* `SAGE_0`: This model is trained using solely synthetically generated TEM images and their corresponding masks (initialized with COCO Weights). \n",
    "* `SAGE_1`: Using the weights from `SAGE_0` for initialization, this model trains on a set of manually segmented real TEM images\n",
    "* `SAGE_2`: Initialized with `SAGE_1` weights, this model trains again on the same real TEM images, but each image is now segmented by a different analyst than in `SAGE_1`.\n",
    "\n",
    "* `COCO_1`: This model is trained in the same manner as `SAGE_1`, except it uses COCO weights for initialization rather than the previously trained synthetic-image-based `SAGE_0` model.\n",
    "* `COCO_2`: Similar to `SAGE_2`, this model is initialized with weights from `COCO_1`, and trained on the same images, segmented by a different analyst than in `COCO_1`.\n",
    "\n",
    "\n",
    "We also subclass our config to create an inference config that can be used to easily change batch size (`GPU_COUNT`*`IMAGES_PER_GPU`) and confidence thresholds (`DETECTION_MIN_CONFIDENCE`). The confidence threshold determines what confidence score is required for a model to view a prediction as \"correct\". For example, if we set a confidence threshold of 0.6, any predictions with a score of 0.6 or higher will be returned as possible match. \n",
    "\n",
    "After specifying model paths, we intialize a dictionary `model_dict` to store the models and relevant information.\n",
    "\n",
    "Next, each model specified in `model_paths` is loaded and registered to the `model_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#here, specify the model paths to be loaded\n",
    "\n",
    "model_paths = [\n",
    "    #os.path.join(PRETRAIN_DIR, \"SAGE_0/SAGE_0.h5\"),\n",
    "    #os.path.join(PRETRAIN_DIR, \"SAGE_1/SAGE_1.h5\"),\n",
    "    os.path.join(PRETRAIN_DIR, \"SAGE_2/SAGE_2.h5\"),\n",
    "    #os.path.join(PRETRAIN_DIR, \"COCO_1/COCO_1.h5\"),\n",
    "    os.path.join(PRETRAIN_DIR, \"COCO_2/COCO_2.h5\")\n",
    "]\n",
    "\n",
    "#subclass config for inference\n",
    "\n",
    "class InferenceConfig(SAGEConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.6 #minimum confidence score for prediction to be accepted\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "#initialize dict and list for model storage/access\n",
    "model_dict = {}\n",
    "model_list = []\n",
    "\n",
    "#load each model in \"model_paths\"\n",
    "for path in model_paths:\n",
    "    utils.load_model(path, model_dict, model_list, MODEL_DIR, inference_config)\n",
    "    \n",
    "    \n",
    "#print list of loaded models    \n",
    "utils.print_active_models(model_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Save Mask Overlays\n",
    "\n",
    "This section provides the ability to save overlays of the masks created by both the model and the loaded dataset. \n",
    "\n",
    "### 3.2a Dataset Masks\n",
    "\n",
    "We can view the list of loaded datasets using `utils.print_loaded_datasets()` to easily fetch the correct name keys of each dataset. \n",
    "\n",
    "To save the overlays of a dataset, select which dataset to save (`dataset_name`) and set boolean `save=True`. If the dataset contains manually segmented images (as opposed to some other imported masks from another segmentation method), set the boolean `Ground_Truth=True`.\n",
    "\n",
    "If you are saving overlays of a dataset containing segmentation masks from imported masks from another segmentation method (to compare against the 'ground truth' manual masks), set `Ground_Truth=False` to ensure it saves within the proper directory. \n",
    "\n",
    "We can also toggle whether we want to view overlays with the boolean `view_overlay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_loaded_datasets(datasets)\n",
    "\n",
    "## Parameters for overlay viewing/saving\n",
    "dataset_name = 'PROCI_Test'\n",
    "dataset_analyze = datasets.get(dataset_name, None)\n",
    "save = False\n",
    "view_overlays = True\n",
    "Ground_Truth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if dataset_analyze and save:\n",
    "    if not Ground_Truth:\n",
    "        vis_dir = os.path.join(Results_DIR,\"PROCI_Test\", \"Visualizations\",dataset_name, \"Display_instances\")\n",
    "    else:\n",
    "        vis_dir = os.path.join(Results_DIR,dataset_name, \"Visualizations\", \"Ground Truths\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "image_ids= dataset_analyze.image_ids\n",
    "for image_id in image_ids:\n",
    "    #print(f\"Image ID: {image_id}\")\n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_analyze, inference_config, \n",
    "                               image_id)\n",
    "\n",
    "    log(\"original_image\", original_image)\n",
    "    log(\"image_meta\", image_meta)\n",
    "    log(\"gt_class_id\", gt_class_id)\n",
    "    log(\"gt_mask\", gt_mask)\n",
    "      \n",
    "    #print(ious_for_image)\n",
    "    original_filename = dataset_analyze.image_info[image_id]['basename']\n",
    "    base_filename = os.path.splitext(os.path.basename(original_filename))[0] \n",
    "    print(f\"Image ID: {image_id}, File Path: {base_filename}\")\n",
    "    if save:    \n",
    "        save_path = os.path.join(vis_dir, f\"{base_filename}_visualization.png\")\n",
    "        print(f\"Saving visualization to {save_path}\")\n",
    "    else:\n",
    "        save_path=None\n",
    "        print(\"saving not activated\")\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset_analyze.class_names, figsize=(8, 8),show_mask=False, show_bbox=False, \n",
    "                                    show_caption=False, linewidth = 2, save_path=save_path, view=view_overlays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2b Model Prediction Masks\n",
    "\n",
    "\n",
    "First, select which model to use for analysis (`model_name=`). We can view the list of loaded models using `utils.print_active_models()` to easily fetch the correct name keys of each model. \n",
    "\n",
    "Next, select which dataset to analyze (`dataset_analyze=`). This will provide the model with images to make predictions on.\n",
    "\n",
    "Similar to saving the dataset masks, setting `save=True` will save images with the masks overlaid, as well as binary images of each prediction mask.\n",
    "\n",
    "Next, choose whether you want to view each overlay with the `view_overlays` boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.print_active_models(model_dict)\n",
    "\n",
    "#Model overlay settings\n",
    "\n",
    "model_name = 'SAGE_0'\n",
    "dataset_name = 'PROCI_Test'\n",
    "save = False\n",
    "view_overlays = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save model predictions as overlays\n",
    "\n",
    "#extract model and confidence threshold from model dict\n",
    "model = model_dict[model_name]['model']\n",
    "threshold = model_dict[model_name]['confidence']\n",
    "\n",
    "#extract images to run model on from dataset\n",
    "dataset_analyze = datasets.get(dataset_name, None)\n",
    "image_ids= dataset_analyze.image_ids\n",
    "\n",
    "#create visualization directory\n",
    "vis_dir = os.path.join(Results_DIR, dataset_name, \"Visualizations\", f\"{model_name}_{threshold}\", \"Display_instances\")\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "#create directory to save individual masks\n",
    "particle_dir = os.path.join(vis_dir, \"particle\")\n",
    "os.makedirs(particle_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model {model_name}'s predictions above {threshold} confidence score on {dataset_name} images\\\\\")\n",
    "\n",
    "for image_id in image_ids:\n",
    "    print(f\"Image ID: {image_id}\")\n",
    "    image = dataset_analyze.load_image(image_id)\n",
    "    mask, class_ids = dataset_analyze.load_mask(image_id)\n",
    "\n",
    "    print(dataset_analyze.class_names)\n",
    "    results = model.detect([image], verbose=1)\n",
    "\n",
    "    r = results[0]\n",
    "    \n",
    "    original_filename = dataset_analyze.image_info[image_id]['basename']\n",
    "    base_filename = os.path.splitext(os.path.basename(original_filename))[0] \n",
    "    base_num = base_filename.split('_')[-1]\n",
    "    print(f\"Image ID: {image_id}, File Path: {base_filename}\")\n",
    "    #print(r)\n",
    "    if save == True:\n",
    "        save_path = os.path.join(vis_dir, f\"{base_filename}_predictions.png\")\n",
    "        print(f\"Saving visualization to {save_path}, saving masks to {particle_dir}\")\n",
    "        \n",
    "        #save particle masks\n",
    "        masks = r['masks']\n",
    "        \n",
    "        for i in range(masks.shape[-1]):\n",
    "            mask = masks[:,:,i].astype(np.uint8) #ensure binary?\n",
    "            img = Image.fromarray(mask*255)\n",
    "            fname = f\"mask_{base_num}_{i:06d}.png\"\n",
    "            img.save(os.path.join(particle_dir,fname))\n",
    "\n",
    "\n",
    "        \n",
    "    else:\n",
    "        save_path = None\n",
    "        print(\"saving not activated\")\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset_analyze.class_names, scores=r['scores'],\n",
    "                                show_caption=False,show_bbox=False, show_mask=True, linewidth=2, save_path=save_path, view=view_overlays)#, ax=get_ax())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 IoU Heatmaps\n",
    "\n",
    "IoU heatmaps provide a per-instance visualization of how well model predictions overlap with ground truth masks. In this section, we generate \"IoU heatmaps\", that color-code each prediction by its IoU score (green = high overlap/good prediction, red = low, pink = unmatched prediction). \n",
    "\n",
    "We start by selecting the **model/method name** (e.g., `SAGE_0` or `PROCI_EDMWSv2`), then the **reference dataset** that supplies the ground truth masks (`PROCI_Test`).\n",
    "\n",
    "Then, we can specify certain settings to pass to the heatmap visualization:\n",
    "* `metric_name` (str): metric to visualize e.g. (IoU)\n",
    "* `normalize_metric` (bool): whether to normalize metric between 0 and 1 (True) or min and max (False)\n",
    "* `show_caption` (bool): display captions or labels on predictions\n",
    "* `show_bbox`(bool): display bounding box for predictions\n",
    "* `show_mask` (bool): display predicted mask overlay (filled in)\n",
    "* `show_cbar` (bool): display colorbar for metric values \n",
    "* `colormap` (str): color scheme for heatmapping (e.g., `'RdYlGn'` - check matplotlib for other options)\n",
    "* `cbar_position`(list): locatiopn and size of colorbar `[x0, y0, width, height]`,\n",
    "* `label_color` (str): color for text labels on masks.\n",
    "* `show_pred_idx` (bool): display prediction index numbers\n",
    "* `unmatched_color`(tuple): RGB color for predicitons that don't match any ground truth mask (e.g, pink)\n",
    "\n",
    "The model is then run on the reference dataset, and the generated heatmaps are displayed and saved to the reference dataset's results directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_active_models(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = 'SAGE_2'\n",
    "ref_name = 'PROCI_Test'\n",
    "model_dict =  model_dict\n",
    "datasets= datasets\n",
    "config = InferenceConfig\n",
    "\n",
    "heatmap_settings = dict(\n",
    "            metric_name=\"IoU\",\n",
    "            normalize_metric=False,\n",
    "            show_caption=False,\n",
    "            show_bbox=False,\n",
    "            show_mask=True,\n",
    "            show_cbar=False,\n",
    "            colormap='RdYlGn',\n",
    "            cbar_position=[0.1, 0.1, 0.8, 0.05],\n",
    "            label_color='black',\n",
    "            show_pred_idx=False,\n",
    "            unmatched_color=(1.0, 105/255.0, 180/255.0)\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "filtered_df, full_name, model, dataset, ref_data = visualize.get_filtered_df(method_name,\n",
    "                                                                   ref_name,\n",
    "                                                                   model_dict, datasets,\n",
    "                                                                   config,\n",
    "                                                                   filter_size=False,\n",
    "                                                                   verbose=False)\n",
    "visualize.get_iou_heatmaps(filtered_df = filtered_df,\n",
    "                ref_data = ref_data, model =model, \n",
    "                 dataset=dataset, method_name = full_name, \n",
    "                 config=config,\n",
    "                 ref_name = ref_name, \n",
    "                 Results_DIR = Results_DIR, \n",
    "                 verbose = False, save_images=False, vis_settings=heatmap_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "The evaluation section of this notebook will allow us to compare the performance of multiple models or loaded segmentation methods through various methods, as well as save performance metrics for later viewing and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Comparing IoU Distributions of multiple models\n",
    "\n",
    "The quality of a model's predictions can be quantified by their Intersection-over-Union (IoU) score. In this section, we compare IoU's by plotting their reverse cumulative IoU distributions against a chosen reference dataset (`PROCI_Test`).\n",
    "* `methods`: list of models or methods (imported datasets) to include in comparison using the name key of loaded models or datasets (e.g., `SAGE_0, SAGE_1, SAGE_2`)\n",
    "* `ref_dataset`: dataset used as the \"ground truth\" for evaluation\n",
    "* `method_styles`: optional dictionary to customize labels and colors for each method in the plot\n",
    "\n",
    "The function `visualize.plot_rev_cum_iou()` generates the plots, allowing us to visually assess how well a model/method overlaps with ground truth masks across the full IoU range. Results are saved in the `IoU Distributions` folder for the chosen reference dataset.\n",
    "\n",
    "Some key options for `visualize.plot_rev_cum_iou()`:\n",
    "* `iou_threshold` (float, default=0): IoU cutoff threshold for processing matches\n",
    "* `iou_summary` (bool, default=False): whether to print an IoU statistics summary for each model passed\n",
    "* `fill` (bool, default=True): whether to fill the area under the curve\n",
    "* `method_styles` (dict or None, default=None): optional custom style settings (label, color) for each method plotted\n",
    "* `title` (bool, default=False): whether to show title on plot\n",
    "* `verbose` (bool, default=False): print additional debug and tracking information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.print_active_models(model_dict)\n",
    "#utils.print_loaded_datasets(datasets)\n",
    "\n",
    "#names of models or datasets you want to compare\n",
    "methods = [ 'SAGE_2', 'COCO_2', 'PROCI_EDMWSv2']\n",
    "\n",
    "#dataset to test models/methods against\n",
    "ref_dataset = 'PROCI_Test'\n",
    "\n",
    "#optional dictionary to define plotting styles for models/methods\n",
    "method_styles = {\n",
    "    'SAGE_2': {\n",
    "        'label': r'SAGE$_2$',\n",
    "        'color': '#2ca02c' #green\n",
    "    },\n",
    "     'COCO_2':{\n",
    "         'label': r'COCO$_2$',\n",
    "        'color': '#ff7f0e' #orange\n",
    "     },\n",
    "    'PROCI_EDMWS':{\n",
    "        'label':'EDM-WS (Full Masks))',\n",
    "        'color':'#1f77b4' #blue\n",
    "    },\n",
    "    'SAGE_1':{\n",
    "        'label':'SAGE_1',\n",
    "        'color': '#d62728' #red\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "                   \n",
    "#save_dir = os.path.join(Results_DIR, ref_dataset,\"Visualizations\", \"IoU Distributions\")\n",
    "save_dir=None\n",
    "visualize.plot_rev_cum_iou(methods,\n",
    "                           model_dict=model_dict,\n",
    "                           dataset_dict=datasets,\n",
    "                           ref_dataset=ref_dataset,\n",
    "                           config=inference_config,\n",
    "                           save_dir=save_dir,\n",
    "                           iou_threshold=0,\n",
    "                           iou_summary=False,\n",
    "                           method_styles=method_styles,\n",
    "                           fill=True,\n",
    "                           title=True,\n",
    "                           verbose=False)\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Postprocessing/Results\n",
    "\n",
    "Here, we will run a selected model (`model_name`) on the desired ground truth dataset (`ref_dataset`) to compile various morphological information and performance metrics, and save them into our results directory \n",
    "\n",
    "(TODO verify/add view only option)\n",
    "\n",
    "After selecting our model and ground truth dataset, we create a `summary_settings` dict to control some overall functions in each postprocessing loop:\n",
    "* `dataset_name` (str): Name of ground truth dataset\n",
    "* `model_name` (str): Name of model to run\n",
    "* `datasets` (dict): previously loaded datasets dictionary \n",
    "* `model_dict` (dict): previously loaded model dictionary\n",
    "* `Results_DIR` (str): path to save metrics (usually predefined)\n",
    "* `verbose` (bool): verbosity flag to print additional information/debug\n",
    "* `save_results` (bool): saves results to Results_DIR\n",
    "\n",
    "Next we will set the scale for our images (nm/pixel). If we are using an image set containing images of different scales, create a `scales` dictionary, and assign the correct scale to each image. If all images in a set have the same scale, we can just set `scales` as a single float value. \n",
    "\n",
    "The postprocessing/results gathering portion is broken into three main functions:\n",
    "* 1. `gather_aggregate_morphology()` is a wrapper function that takes each image and determines various morphological information about the aggregate and primary particles in an image. This will run images through a pipeline to determine morphological information such as # of primary particles, mean primary particle diameter, radius of gyration, and fractal dimension of aggregates, returning a dataframe summarizing aggregate information (`aggregate_summary`) as well as a dataframe containing individual primary particle information (`pp_info`) a .csv file in the results directory. If saving is enabled in `summary_settings`, it will also save these into .csv files in the results directory for the corresponding reference dataset and model. This function is useful for analyzing datasets with or without ground truth particle masks, as it returns morphology information depending on the provided predictions, not ground truth labels. \n",
    "\n",
    "* 2. `calc_performance_metrics()` will run the model on the reference dataset, returning key machine learning performance metrics, such as confusion matrix values (TP,FP, FN), Accuracy, F<sub>1</sub> score, Average Precisions, and mean IoU. Enabling `save_results` in the summary settings will save these metrics to a csv file containing the metrics for all models/methods run on a reference dataset. This function is useful when determing how accurate a model is against a ground truth dataset. If no ground truth information is available, there is no need to run this function. \n",
    "\n",
    "* 3. `full_summary()` will take the outputs of `gather_aggregate_morphology()` and `calc_performance_metrics()`, returning (and saving) a full summary of both performance and morphological information.  \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose Model/method\n",
    "model_name = 'COCO_2' #(str or None)\n",
    "dataset_name = 'PROCI_Test' #(str or None) - use this if analyzing dataset of imported method\n",
    "ref_name = 'PROCI_Test'#'NS40_test' #why do I have both ref_name and dataset_name?\n",
    "\n",
    "\n",
    "#settings for postprocessing/results gathering\n",
    "summary_settings = dict(dataset_name = ref_name, #dataset name\n",
    "                        model_name = model_name, #model name\n",
    "                        datasets = datasets, #loaded datasets dict\n",
    "                        model_dict=model_dict, # dict of models\n",
    "                        Results_DIR = Results_DIR, #results directory\n",
    "                        verbose = 1, #verbose toggle\n",
    "                        save_results = False) #saving toggle\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#TODO - check if single scale can be passed instead of dict\n",
    "#TODO - double check unit\n",
    "\n",
    "\n",
    "scales = {\n",
    "    'image_000007':  0.2847, #scale of kunfeng images\n",
    "    'image_000010': 0.16775516, #scale of dreier images\n",
    "    'image_000017': 0.2847,\n",
    "    'image_000020': 0.16775516,\n",
    "    'image_000024': 0.16775516,\n",
    "    'image_000028': 0.2847,\n",
    "    'image_000043': 0.2847,\n",
    "    'image_000049': 0.2847,\n",
    "    'image_000053': 0.2847,\n",
    "    \n",
    "}\n",
    "\n",
    "#or\n",
    "#scales = 0.2847 float value if all images have same scale \n",
    "\n",
    "aggregate_summary, pp_info = utils.gather_aggregate_morphology(ref_name,\n",
    "                                                               scales,\n",
    "                                                               summary_settings,\n",
    "                                                               save_binary=False,\n",
    "                                                               show_binary=False, \n",
    "                                                               plot=False)\n",
    "\n",
    "metrics = utils.calc_performance_metrics(ref_name, \n",
    "                                         inference_config,\n",
    "                                         summary_settings,\n",
    "                                         sort_method='confidence')\n",
    "summary = utils.full_summary(aggregate_summary, metrics, pp_info, summary_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAGE-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
